{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf946f9c",
   "metadata": {},
   "source": [
    "# Linear Regression with Polynomial Basis Expansion\n",
    "\n",
    "\n",
    "### First task\n",
    "\n",
    "The first task is to have a go a coding up a polynomial basis expansion for simple linear regression from scratch, you can try different degree polynomials to find the best fitting model on some synthetic data.\n",
    "\n",
    "### Second task\n",
    "\n",
    "The second task is to try and come up with the best fitting model for the white wine dataset which we used a few weeks ago.\n",
    "\n",
    "Download the pickle file ```winequality-white.pickle``` and save it in this new data folder.\n",
    "\n",
    "I have given you some starter code so that you can use polynomial basis expansion and L1 or L2 regularization, everything in this part should be implemented with scitkit-learn. If you want you can try more advanced regression techniques available in scikit-learn. Although, remember the goal is to find the model that has the best accuracy on the test data set on the training data set.\n",
    "\n",
    "For more details about scikit-learn see : (https://scikit-learn.org/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e825e91a",
   "metadata": {},
   "source": [
    "# Polynomial features with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc11104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the imports we need (DONT CHANGE THIS CODE!)\n",
    "import matplotlib.pyplot as plt\n",
    "import _pickle as cp\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb6d3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated Data (DONT CHANGE THIS CODE!)\n",
    "np.random.seed(1)\n",
    "N = 10\n",
    "f = lambda x: (x-0.5)*(x-1.0)*(x-2.5)*(x-4.0)\n",
    "x = np.random.uniform(0.0, 5.0, size=(N,))\n",
    "#x = np.linspace(0.0, 5.0, num=N)\n",
    "y = f(x) + np.random.randn(N)*0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057f8fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data points are stored in the numpy arrays x and y (DONT CHANGE THIS CODE!)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea04409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remeber we are trying to predict y given x, let's plot the datapoints out (DONT CHANGE THIS CODE!)\n",
    "plt.scatter(x, y, color='blue', label='Data Points') \n",
    "plt.xlabel('X values')\n",
    "plt.ylabel('Y values')\n",
    "plt.title('Scatter Plot the Datapoints')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522aaf17",
   "metadata": {},
   "source": [
    "### Now its your turn\n",
    "\n",
    "Now it is up, first you need to transform the data matrix to include the bias term $[1, ..., 1]$ the datapoints $[x_1, x_2, ..., x_N]$ and the polynomial features $[x^2_1, ..., x^2_N]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d644676",
   "metadata": {},
   "source": [
    "### Some helper functions\n",
    "\n",
    "These functions should be all you need to create the polynomial feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc5e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an an array of all zeros\n",
    "print(np.ones(5, dtype=np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e43c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# elementwise square of an array - this works for python lists and numpy arrays\n",
    "print(np.power([0.1, 2.0, 3.0, -2.0], 2))\n",
    "# you can also use **2 notation for numpy arrays (THIS WON\"T WORK FOR PYTHON LISTS) - DONT USE ^2 this is a common mistake\n",
    "a = np.array([0.1, 2.0, 3.0, -2.0])\n",
    "print(a**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f75070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack two or more arrays together\n",
    "a = np.array([0.1, 2.0, 3.0, -2.0])\n",
    "b = np.array([0.3, -3.0, 6.0, -0.5])\n",
    "c = np.array([-0.7, 3.0, 0.1, 0.3])\n",
    "print(np.stack([a,b], axis=1))\n",
    "print(np.stack([a,b,c], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cb7133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now you need to create the X data matrix with polynomial features\n",
    "\n",
    "# degree of the polynomial\n",
    "p = 2\n",
    "\n",
    "# construct the new data matrix with polynomial features\n",
    "X = ?\n",
    "\n",
    "# the shape of the new data matrix should be (N, p+1)\n",
    "assert X.shape == (N, p+1)\n",
    "\n",
    "N, D = X.shape\n",
    "\n",
    "print(f'data matrix has {N} data points, with {D} features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e388fe",
   "metadata": {},
   "source": [
    "### Now it's your turn [Exercise]\n",
    "\n",
    "Now it is up to you to derive the least squares estimate for the data matrix ```X``` and ```y``` using only numpy commands (we did this a few weeks ago you can re-use your code).\n",
    "\n",
    "- To invert a matrix use ```inv(matrix)```\n",
    "- To multiple two matrices together use ```np.matmul(matrix_1, matrix_2)```\n",
    "- To transpose a matrix use ```np.transpose(matrix)``` or ```matrix.transpose()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594950cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n",
    "# compute the weights\n",
    "weights = ?\n",
    "\n",
    "assert weights.shape == (p+1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be810b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the weights of the model \n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1830bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute the mean squared error of this model (DONT CHANGE THIS CODE!)\n",
    "y_hat = np.polyval(np.flip(weights), x)\n",
    "mse = ((y - y_hat)**2).mean()\n",
    "print(f\"You model has mean squared error {mse}, can you do better?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf2e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the result - let's see how well your line fits the data (DONT CHANGE THIS CODE!)\n",
    "plt.scatter(x, y, color='blue', label='Data Points')\n",
    "x_line = np.linspace(min(x), max(x), 1000)  # Generate 100 points along x for the line\n",
    "y_line = np.polyval(np.flip(weights), x_line)\n",
    "plt.plot(x_line, y_line, color='red')  # Line plot\n",
    "\n",
    "plt.xlabel('X values')\n",
    "plt.ylabel('Y values')\n",
    "plt.axhline(0, color='black', linewidth=0.5, linestyle='--')  # x-axis\n",
    "plt.axvline(0, color='black', linewidth=0.5, linestyle='--')  # y-axis\n",
    "plt.title('Scatter Plot with Line')\n",
    "\n",
    "plt.ylim(min(y)-1, max(y)+1) \n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d713d7a5",
   "metadata": {},
   "source": [
    "# Task 2 - polynomial features with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8877bf",
   "metadata": {},
   "source": [
    "### Loading in the data\n",
    "\n",
    "First we load in the data (feature) matrix ```X``` and the data (output) vector ```y```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3983dd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the full dataset (DONT CHANGE THIS CODE!)\n",
    "# data matrix X and data vector y\n",
    "X, y = cp.load(open('data/winequality-white.pickle', 'rb'))\n",
    "# print the shape of the dataset\n",
    "print(f'data matrix shape {X.shape}', f'data vector shape {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d9132",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "First we need to do some simple data preprocessing. Remember we need to add a column of all ones to model the bias vector giving us a data matrix with shape N, D+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837b3d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N refers to the number of datapoints, D refers to the dimension of each datapoint (how many features it has)\n",
    "N, D = X.shape\n",
    "\n",
    "# (DONT CHANGE THIS CODE!) this line adds a column of all ones to the front of the data matrix\n",
    "print(f'data matrix has {N} data points, with {D} features')\n",
    "X = np.concatenate([np.ones(N).reshape(-1,1), X], axis=1)\n",
    "\n",
    "N, D = X.shape\n",
    "print(f'data matrix has {N} data points, with {D} features')\n",
    "\n",
    "# The first column of X is now entire ones, uncomment the following print statement to see.\n",
    "#print(X[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cfd883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N refers to the number of datapoints, D refers to the dimension of each datapoint (how many features it has)\n",
    "N, D = X.shape\n",
    "\n",
    "# split the data set into 80% train and 20% test data (DONT CHANGE THIS CODE!)\n",
    "N_train = int(0.8 * N)\n",
    "N_test = N - N_train\n",
    "\n",
    "X_train = X[:N_train]\n",
    "y_train = y[:N_train]\n",
    "X_test = X[N_train:]\n",
    "y_test = y[N_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938ca17f",
   "metadata": {},
   "source": [
    "### Using scikit-learn\n",
    "\n",
    "Like before we can use scikit-learn to do multiple linear regression for us under the hood.\n",
    "\n",
    "Have a play around with lasso (L1) and ridge (L2) regression and the corresponding lambda/alpha hyperparameter which weights the regularization, what happens if you set alpha = 100? What do the weights look like? What is the best configuration that you found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5ca00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard linear regression without regularization\n",
    "reg = linear_model.LinearRegression()\n",
    "\n",
    "# linear regression with L1 regularization\n",
    "reg = linear_model.Lasso(alpha=0.1)\n",
    "\n",
    "# linear regression with L2 regularization\n",
    "reg = linear_model.Ridge(alpha=0.1)\n",
    "\n",
    "# fit a linear model to the training data\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# scikit learn stores the feature weights and bias (y-intercept) in different attributes\n",
    "# we just need to reconcile them into one vector\n",
    "weights = np.array(reg.coef_)\n",
    "weights[0] = np.array(reg.intercept_)\n",
    "\n",
    "# compute the y hat estimates for the train data \n",
    "y_hat_train = np.matmul(X_train, weights)\n",
    "# compute the y hat estimates for the test data\n",
    "y_hat_test = np.matmul(X_test, weights)\n",
    "\n",
    "# compute the training dataset error\n",
    "train_mse = ((y_train - y_hat_train)**2).mean()\n",
    "# compute the test dataset error\n",
    "test_mse = ((y_test - y_hat_test)**2).mean()\n",
    "\n",
    "# print out the train and test error to see if the model has overfit or not\n",
    "print(\"Train Error (MSE): {:.4f}, Test Error (MSE): {:.4f}\".format(train_mse, test_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0a7389",
   "metadata": {},
   "source": [
    "### Optional extra [Exercise]\n",
    "\n",
    "Now try using polynomial features from scikit-learn.\n",
    "\n",
    "Can you get a better train and test error than standard multiple linear regression? Play around with the degree of the polynomial ```p``` don't make it too large! Also have a play around with Lasso (L1) and Ridge (L2) regression, this might improve things even more!!\n",
    "\n",
    "See (https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) for extra details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c68d1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 2 # degree of the polynomial\n",
    "poly = PolynomialFeatures(p)\n",
    "\n",
    "# transform the train and test data matrix with polynomial features\n",
    "poly_X_train = poly.fit_transform(X_train)\n",
    "poly_X_test = poly.fit_transform(X_test)\n",
    "\n",
    "# print out how many features we now have\n",
    "N, D = X.shape\n",
    "print(f'data matrix has {N} data points, with {D} features')\n",
    "\n",
    "# standard linear regression without regularization\n",
    "reg = linear_model.LinearRegression()\n",
    "\n",
    "# fit a linear model to the training data\n",
    "reg.fit(poly_X_train, y_train)\n",
    "\n",
    "# scikit learn stores the feature weights and bias (y-intercept) in different attributes\n",
    "# we just need to reconcile them into one vector\n",
    "weights = np.array(reg.coef_)\n",
    "weights[0] = np.array(reg.intercept_)\n",
    "\n",
    "# compute the y hat estimates for the train data \n",
    "y_hat_train = np.matmul(poly_X_train, weights)\n",
    "# compute the y hat estimates for the test data\n",
    "y_hat_test = np.matmul(poly_X_test, weights)\n",
    "\n",
    "# compute the training dataset error\n",
    "train_mse = ((y_train - y_hat_train)**2).mean()\n",
    "# compute the test dataset error\n",
    "test_mse = ((y_test - y_hat_test)**2).mean()\n",
    "\n",
    "# print out the train and test error to see if the model has overfit or not\n",
    "print(\"Train Error (MSE): {:.4f}, Test Error (MSE): {:.4f}\".format(train_mse, test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de119f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
