{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77148148",
   "metadata": {},
   "source": [
    "# Feed forward networks - universal approximation theorem\n",
    "\n",
    "This goal of this notebook is to demonstrate the universal approximation theorem for feed forward neural networks. We will use a synthetic data set generated from the non-linear function $f(x) = 0.1 x^3 - x^2 + x + 1$, after only a few epochs of training the neural network should closely approximate this function with a piece-wise linear function.\n",
    "\n",
    "For a nice visual proof and discussion of the universal approximation theorem please refer to this youtube video: [Universal Approximation with Deep Narrow Networks](https://www.youtube.com/watch?v=6-SORkCLYPM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# setting the random seed - DON'T CHANGE THIS\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba0070f",
   "metadata": {},
   "source": [
    "### Generate the synthetic data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e428179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target function f(x) = x^3 + x^2 - x - 1\n",
    "# [TODO]: feel free to modify the target function as you see fit\n",
    "def target_function(x):\n",
    "    return 0.1 * x**3 - x**2 + x + 1\n",
    "\n",
    "# Generate 1000 training examples in the range [-10, 10]\n",
    "# [TODO]: feel free to modify the number of training examples and see if the accuracy of the model improves or not.\n",
    "n_samples = 1000\n",
    "x_train = np.random.uniform(-5.0, 10.0, n_samples)\n",
    "# wrap the training data in a torch tensor for training on the GPU\n",
    "x_train = torch.tensor(x_train, dtype=torch.get_default_dtype())\n",
    "y_train = target_function(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95279cae",
   "metadata": {},
   "source": [
    "### Define the architecture of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d749fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CLASS DEFINITION\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_size, hidden_size))\n",
    "        layers.append(nn.ReLU())\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_size, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instantiating the MLP\n",
    "# TODO: feel free to modify the width and depth of the network\n",
    "hidden_size = 64  # width of the hidden layers\n",
    "num_layers = 2    # depth of the network\n",
    "model = MLP(hidden_size=hidden_size, num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e21ad45",
   "metadata": {},
   "source": [
    "### Define the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b54d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for regression tasks we usually use mean squared error\n",
    "loss_fn = nn.MSELoss()\n",
    "# Adam is a more sophisticated version of gradient descent, that implements momentum and second order (Newtonian) moments\n",
    "# It works well for most tasks and is the de facto optimizer for neural networks\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4035f5",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "The inner training loop is structed as follows:\n",
    "- Sample a batch of data from the dataset.\n",
    "- Evaluate the model for this batch of data to get the predictions.\n",
    "- Compute the loss of the predictions with respect to the ground truth labels.\n",
    "- Backpropagate the gradients through the model.\n",
    "- Update the parameters of the network in the negative direction of the gradients (minimize). \n",
    "\n",
    "We repeat this until all the data in the dataset has been seen, this corresponds to one epoch of training. We can run training for many epochs until we see the training loss converge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5451cc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "# TODO: feel free to play around with the hyperparameters;\n",
    "# How does the batch size affect learning? \n",
    "# How many epochs do we need for the model be accurate?\n",
    "batch_size = 64\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # shuffle the dataset\n",
    "    indices = torch.randperm(n_samples)\n",
    "    x_train_shuffled = x_train[indices]\n",
    "    y_train_shuffled = y_train[indices]\n",
    "    \n",
    "    # iterate over the batches\n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        \n",
    "        # create batches of the training data\n",
    "        x_batch = x_train_shuffled[i:i+batch_size].unsqueeze(1)\n",
    "        y_batch = y_train_shuffled[i:i+batch_size].unsqueeze(1)\n",
    "        \n",
    "        # forward pass\n",
    "        y_pred = model(x_batch)  # predictions from the model\n",
    "        \n",
    "        # compute the loss\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        \n",
    "        # backpropagate the loss\n",
    "        optimizer.zero_grad() # first we zero any gradients from the previous iteration\n",
    "        loss.backward() # backpropagate \n",
    "        \n",
    "        # update the parameters of the MLP\n",
    "        optimizer.step()\n",
    "        \n",
    "    # print loss every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{n_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a6ee9",
   "metadata": {},
   "source": [
    "### Evaluating the model\n",
    "\n",
    "Now we will evaluate the model by making predictions at 10 equally spaced points in the range $[-5, 10]$. This will help us visualize the piecewise linear function learned by the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d8ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 equally spaced points\n",
    "x_eval = torch.linspace(-5, 10, 10).unsqueeze(1)\n",
    "# this turns of any gradient computations - we don't need to compute the gradients when evaluating the model\n",
    "with torch.no_grad():\n",
    "    y_eval = model(x_eval)  # predictions from the trained model\n",
    "    \n",
    "# take the predictions off the GPU and convert to numpy arrays\n",
    "x_piecewise = x_eval.numpy()\n",
    "y_piecewise = y_eval.numpy()\n",
    "\n",
    "# evlaute the ground truth function\n",
    "x_truth = np.linspace(-5, 10, 1000)\n",
    "y_truth = target_function(x_truth)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_truth, y_truth, label=\"Original function f(x)\", color='blue', alpha=0.6)\n",
    "plt.plot(x_piecewise, y_piecewise, label=\"Piecewise Linear Approximation\", color='red', linestyle='--')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc59021-060a-4bd8-845b-adc3634561f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
