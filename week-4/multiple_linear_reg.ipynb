{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57295987",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "This week we will be get our hands on some real data and create a multiple linear regression model from the data. First you need to create a new directory called ```data``` which we will use to store the data. In the command line run:\n",
    "\n",
    "```mkdir data```\n",
    "\n",
    "Download the pickle file ```winequality-white.pickle``` and save it in this new data folder.\n",
    "\n",
    "We will use scikit-learn to do the linear regression for us, then there will be a little exercise for you to use numpy to compute the least squares estimate and compare wether your code gives the same set of weights as scikit-learn.\n",
    "\n",
    "For more details about scikit-learn see : (https://scikit-learn.org/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f6a2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the imports we need\n",
    "import matplotlib.pyplot as plt\n",
    "import _pickle as cp\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3340b7bc",
   "metadata": {},
   "source": [
    "### Loading in the data\n",
    "\n",
    "First we load in the data (feature) matrix ```X``` and the data (output) vector ```y```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ee0b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the full dataset\n",
    "# data matrix X and data vector y\n",
    "X, y = cp.load(open('data/winequality-white.pickle', 'rb'))\n",
    "# print the shape of the dataset\n",
    "print(f'data matrix shape {X.shape}', f'data vector shape {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb59015",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "First we need to do some simple data preprocessing. Remember we need to add a column of all ones to model the bias vector giving us a data matrix with shape N, D+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d9d952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N refers to the number of datapoints, D refers to the dimension of each datapoint (how many features it has)\n",
    "N, D = X.shape\n",
    "\n",
    "print(f'data matrix has {N} data points, with {D} features')\n",
    "X = np.concatenate([np.ones(N).reshape(-1,1), X], axis=1)\n",
    "\n",
    "N, D = X.shape\n",
    "print(f'data matrix has {N} data points, with {D} features')\n",
    "\n",
    "# The first column of X is now entire ones, uncomment the following print statement to see.\n",
    "#print(X[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d20baea",
   "metadata": {},
   "source": [
    "### Splitting the data up\n",
    "\n",
    "It is standard practice to split up your dataset into training data and test data. The training data is used to train your model, or in this instance calculate the least squares estimate, the test data is then kept aside to evaluate your model on unseen data. This is very important! As if there is a big discrepency and accuracy between the training and test data then this indicated you have overfit to your training data. If this is the case then you might need to consider a different type of model or use additional *regularization* techniques to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45a8a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N refers to the number of datapoints, D refers to the dimension of each datapoint (how many features it has)\n",
    "N, D = X.shape\n",
    "\n",
    "# split the data set into 80% train and 20% test data\n",
    "N_train = int(0.8 * N)\n",
    "N_test = N - N_train\n",
    "\n",
    "X_train = X[:N_train]\n",
    "y_train = y[:N_train]\n",
    "X_test = X[N_train:]\n",
    "y_test = y[N_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e028ce",
   "metadata": {},
   "source": [
    "### Using scikit-learn\n",
    "\n",
    "We can use scikit-learn to do multiple linear regression for us under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11463ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = linear_model.LinearRegression()\n",
    "\n",
    "# fit a linear model to the training data\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# scikit learn stores the feature weights and bias (y-intercept) in different attributes\n",
    "# we just need to reconcile them into one vector\n",
    "weights = np.array(reg.coef_)\n",
    "weights[0] = np.array(reg.intercept_)\n",
    "\n",
    "# copying the weights for later\n",
    "scikit_learn_weights = np.copy(weights)\n",
    "\n",
    "# compute the y hat estimates for the train data \n",
    "y_hat_train = np.matmul(X_train, weights)\n",
    "# compute the y hat estimates for the test data\n",
    "y_hat_test = np.matmul(X_test, weights)\n",
    "\n",
    "# compute the training dataset error\n",
    "train_mse = ((y_train - y_hat_train)**2).mean()\n",
    "# compute the test dataset error\n",
    "test_mse = ((y_test - y_hat_test)**2).mean()\n",
    "\n",
    "# print out the train and test error to see if the model has overfit or not\n",
    "print(\"Train Error (MSE): {:.4f}, Test Error (MSE): {:.4f}\".format(train_mse, test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f52e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the weights of the model\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4373731b",
   "metadata": {},
   "source": [
    "### Now its your turn [Exercise]\n",
    "\n",
    "Now it is up to you to derive the least squares estimate for the training data ```X_train``` and ```y_train``` using only numpy commands.\n",
    "\n",
    "If you get it right then you should get the same values as the weights output by scikit-learn\n",
    "\n",
    "- To invert a matrix use ```inv(matrix)```\n",
    "- To multiple two matrices together use ```np.matmul(matrix_1, matrix_2)```\n",
    "- To transpose a matrix use ```np.transpose(matrix)``` or ```matrix.transpose()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330bbbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n",
    "# weights = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a562fa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the weights of the model \n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058591b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.sum(weights - scikit_learn_weights) < 1e-10:\n",
    "    print(\"Well done the weights you computed are correct!!!\")\n",
    "else:\n",
    "    print(\"The weights you computed are not quite right try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8701d6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the y hat estimates for the train data\n",
    "y_hat_train = np.matmul(X_train, weights)\n",
    "# compute the y hat estimates for the test data\n",
    "y_hat_test = np.matmul(X_test, weights)\n",
    "\n",
    "# compute the training dataset error\n",
    "train_mse = ((y_train - y_hat_train)**2).mean()\n",
    "# compute the test dataset error\n",
    "test_mse = ((y_test - y_hat_test)**2).mean()\n",
    "\n",
    "# the train error and test error should be the same as before if you've correctly computed the same weights (duh!)\n",
    "print(\"Train Error (MSE): {:.4f}, Test Error (MSE): {:.4f}\".format(train_mse, test_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d46762",
   "metadata": {},
   "source": [
    "## Additional exercises [centering the data]\n",
    "\n",
    "For regression it is very common to first center the data around 0 with standard deviation 1. This can help massively with the numerical stability of matrix inversions, understanding the relative effect of each weight and for effective hyperparameter tuning of certain regularization techniques.\n",
    "\n",
    "If you still have time I would like you to try to first center the data around 0 and normalize the data so it has standard deviation 1. You might have seen this before, but a data point ```y``` can be centred by first subtracting the data mean and then dividing by the data standard deviation.\n",
    "\n",
    "```y_centred = (y - y_mean) / y_std```\n",
    "\n",
    "You need to center the data for both the (output) data vector ```y``` and each of the D features in the (feature) data matrix ```X``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3882778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's reload the data in\n",
    "# load the full dataset\n",
    "# data matrix X and data vector y\n",
    "X, y = cp.load(open('data/winequality-white.pickle', 'rb'))\n",
    "# print the shape of the dataset\n",
    "print(f'data matrix shape {X.shape}', f'data vector shape {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c68d6f",
   "metadata": {},
   "source": [
    "#### No bias / y-intercept is needed\n",
    "\n",
    "Since the (output) data vector ```y``` will have mean 0 we no longer need a bias or y-intercept "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf3e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's split up the data again into training and test datasets\n",
    "# N refers to the number of datapoints, D refers to the dimension of each datapoint (how many features it has)\n",
    "N, D = X.shape\n",
    "\n",
    "# split the data set into 80% train and 20% test data\n",
    "N_train = int(0.8 * N)\n",
    "N_test = N - N_train\n",
    "\n",
    "X_train = X[:N_train]\n",
    "y_train = y[:N_train]\n",
    "X_test = X[N_train:]\n",
    "y_test = y[N_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8ca28d",
   "metadata": {},
   "source": [
    "### Now its your turn [Exercise]\n",
    "\n",
    "Now its up to you to center the data given the means and standard deviations of the output feature and all the input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356f2a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compute the mean and standard deviation of the training data\n",
    "# you can use np.mean(matrix) or matrix.mean(), similarly np.std(matrix) or matrix.std()\n",
    "# first for the output feature\n",
    "\n",
    "# your code goes here\n",
    "\n",
    "#y_mean = ?\n",
    "#y_std = ?\n",
    "\n",
    "# now for the input features\n",
    "#X_mean = ?\n",
    "#X_std = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e7333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n",
    "#y_train_centered = ?\n",
    "#y_test_centered = ?\n",
    "\n",
    "#X_train_centered = ?\n",
    "#X_test_centered = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a819fbe2",
   "metadata": {},
   "source": [
    "### Using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59583de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = linear_model.LinearRegression()\n",
    "\n",
    "# fit a linear model to the centered training data\n",
    "reg.fit(X_train_centered, y_train_centered)\n",
    "\n",
    "# we don't need to reconcile the feature weights and bias/y-intercept anymore since the data is centered\n",
    "weights = np.array(reg.coef_)\n",
    "\n",
    "# copying the weights for later\n",
    "scikit_learn_weights = np.copy(weights)\n",
    "\n",
    "# compute the y hat estimates for the train data \n",
    "# we need to unscale our predictions back to the range of our original data\n",
    "y_hat_train = np.matmul(X_train_centered, weights) * y_std + y_mean\n",
    "# compute the y hat estimates for the test data\n",
    "y_hat_test = np.matmul(X_test_centered, weights) * y_std + y_mean\n",
    "\n",
    "# compute the training dataset error\n",
    "train_mse = ((y_train - y_hat_train)**2).mean()\n",
    "# compute the test dataset error\n",
    "test_mse = ((y_test - y_hat_test)**2).mean()\n",
    "\n",
    "# print out the train and test error to see if the model has overfit or not\n",
    "print(\"Train Error (MSE): {:.4f}, Test Error (MSE): {:.4f}\".format(train_mse, test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ae8933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the weights of the model \n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5776fb",
   "metadata": {},
   "source": [
    "### Using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce3decb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n",
    "# weights = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875c139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the weights of the model \n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16919b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.sum(weights - scikit_learn_weights) < 1e-10:\n",
    "    print(\"Well done the weights you computed are correct!!!\")\n",
    "else:\n",
    "    print(\"The weights you computed are not quite right try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e77ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the y hat estimates for the train data\n",
    "y_hat_train = np.matmul(X_train_centered, weights) * y_std + y_mean\n",
    "# compute the y hat estimates for the test data\n",
    "y_hat_test = np.matmul(X_test_centered, weights) * y_std + y_mean\n",
    "\n",
    "# compute the training dataset error\n",
    "train_mse = ((y_train - y_hat_train)**2).mean()\n",
    "# compute the test dataset error\n",
    "test_mse = ((y_test - y_hat_test)**2).mean()\n",
    "\n",
    "# the train error and test error should be the same as before if you've correctly computed the same weights (duh!)\n",
    "print(\"Train Error (MSE): {:.4f}, Test Error (MSE): {:.4f}\".format(train_mse, test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfcabb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
