{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f9aecc4",
   "metadata": {},
   "source": [
    "# Binary Classification with Logistic Regression\n",
    "\n",
    "In this week's notebook the goal is to train a binary classification model using logistic regression. Please do not modify the section of code with a comment ```DO NOT MODIFY``` otherwise you might get different results.\n",
    "\n",
    "The goal is to not use scikit-learn for the first part and implement gradient descent in numpy as an exercise. Then we will compare the result that scikit-learn gives us and discuss what the differences are in our implementation.\n",
    "\n",
    "See (https://scikit-learn.org/stable/) for more details about scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17c5ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate synthetic data\n",
    "def generate_data(n_samples=100):\n",
    "    X = np.random.randn(n_samples, 2)  # 2 features\n",
    "    y = (X[:, 0] + X[:, 1] > 0).astype(int)  # Class 1 if sum of features > 0, else class 0\n",
    "    return X, y\n",
    "\n",
    "# Generate training data\n",
    "X_train, y_train = generate_data(100)\n",
    "\n",
    "# Plot the dataset\n",
    "plt.scatter(X_train[y_train == 0][:, 0], X_train[y_train == 0][:, 1], label='Class 0', color='red')\n",
    "plt.scatter(X_train[y_train == 1][:, 0], X_train[y_train == 1][:, 1], label='Class 1', color='blue')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.title('Synthetic Binary Classification Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291c2a14",
   "metadata": {},
   "source": [
    "### Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def predict(X, w):\n",
    "    # Linear combination: X * w\n",
    "    z = np.dot(X, w)\n",
    "    # Apply sigmoid function to get the probability\n",
    "    return sigmoid(z)\n",
    "\n",
    "def predict_class(X, w):\n",
    "    probabilities = predict(X, w)\n",
    "    return (probabilities >= 0.5).astype(int)  # Class 1 if prob >= 0.5, else class 0\n",
    "\n",
    "def compute_gradient(X, y, w):\n",
    "    # Predicted probabilities\n",
    "    predictions = predict(X, w)\n",
    "    \n",
    "    # Gradient: (1/N) * sum((predictions - y) * X)\n",
    "    gradient = np.dot(np.transpose(X), (predictions - y)) / len(y)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712d2efa",
   "metadata": {},
   "source": [
    "### Implementing gradient descent (exercise)\n",
    "\n",
    "Now it is up to you to implement gradient descent, you need to iteratively apply the following update rule to the weights $\\mathbf{w}$ and return them from the following function \n",
    "\n",
    "$$ \\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta \\cdot \\nabla \\mathcal{L}_{\\mathbf{w}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061a4d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_init, learning_rate=0.1, num_iterations=1000):\n",
    "    w = w_init\n",
    "    \n",
    "    # TODO - you need to implement gradient descent which iteratively updated the weights w by subracting the gradient with small steps\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Compute the gradient\n",
    "        gradient = compute_gradient(X, y, w)\n",
    "        # Update weights using the gradient descent rule\n",
    "        w -= learning_rate * gradient\n",
    "        \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42388862",
   "metadata": {},
   "source": [
    "### Training the logistic regression model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94675381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a bias term (intercept) to the dataset - DO NOT MODIFY THIS LINE OF CODE\n",
    "X_train_bias = np.concatenate([np.ones(X_train.shape[0]).reshape(-1, 1), X_train], axis=1)  # Add a column of ones to X\n",
    "\n",
    "# Initialize weights to zero (with the same size as the number of features + 1 for the bias)\n",
    "# You can play around with different initializations of the weights, all zeros, all ones, sampled from a normal distribution? \n",
    "# w_init = np.ones(X_train_bias.shape[1])\n",
    "# w_init = np.random.randn(X_train_bias.shape[1])\n",
    "w_init = np.zeros(X_train_bias.shape[1])\n",
    "\n",
    "# Train the model using gradient descent\n",
    "# You can play around with the learning rate and number of iterations, its up to you\n",
    "w_opt = gradient_descent(X_train_bias, y_train, w_init, learning_rate=0.1, num_iterations=1000)\n",
    "\n",
    "# Print the optimized weights\n",
    "print(\"Optimized weights:\", w_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6abd1f9",
   "metadata": {},
   "source": [
    "### Making predicitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caec32aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the training set\n",
    "y_pred = predict_class(X_train_bias, w_opt)\n",
    "\n",
    "# Print some of the predictions\n",
    "print(\"Predictions:\", y_pred[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce994b28",
   "metadata": {},
   "source": [
    "### Computing the confusion matrix (exercise)\n",
    "\n",
    "Now it is up to you to compute the confusion matrix using the formulas from the notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3d446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_true, y_pred):\n",
    "    # True positives (TP), false positives (FP), false negatives (FN), true negatives (TN)\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    \n",
    "    return TP, FP, FN, TN\n",
    "\n",
    "# Compute the confusion matrix\n",
    "TP, FP, FN, TN = confusion_matrix(y_train, y_pred)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(f\"TP: {TP}, FP: {FP}, FN: {FN}, TN: {TN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcc3c11",
   "metadata": {},
   "source": [
    "### Computing the performance metrics (exercise)\n",
    "\n",
    "Now it is up to you to compute the performance metrics, precision, recall and accuracy using the formulas from the notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bb246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(TP, FP, FN, TN):\n",
    "    accuracy = (TP + TN) / (TP + FP + FN + TN)\n",
    "    precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    return accuracy, precision, recall\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy, precision, recall = compute_metrics(TP, FP, FN, TN)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710c040f",
   "metadata": {},
   "source": [
    "### Plotting the decision boundary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc87309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, y, y_pred, w):\n",
    "    # Create a grid of points\n",
    "    h = .02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Predict the class label for each point on the grid\n",
    "    Z = predict_class(np.c_[np.ones(xx.ravel().shape[0]), xx.ravel(), yy.ravel()], w)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot the contour and data points\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y_pred, edgecolors='k', marker='o', cmap=plt.cm.RdYlBu)\n",
    "    plt.title(\"Logistic Regression Decision Boundary\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot the decision boundary\n",
    "plot_decision_boundary(X_train, y_train, y_pred, w_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fa7836",
   "metadata": {},
   "source": [
    "### Using scikit-learn\n",
    "\n",
    "Now you can use scikit-learn to do the logistic regression for you - make sure everything else is working up until this point before running the following code. Do you see any differences in results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8969f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "\n",
    "w_sklearn = np.concatenate([model.intercept_.reshape(-1, 1), model.coef_], axis=1)[0]\n",
    "\n",
    "y_pred_sklearn = model.predict(X_train)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "TP, FP, FN, TN = confusion_matrix(y_train, y_pred_sklearn)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(f\"TP: {TP}, FP: {FP}, FN: {FN}, TN: {TN}\")\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy, precision, recall = compute_metrics(TP, FP, FN, TN)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c321d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary\n",
    "plot_decision_boundary(X_train, y_train, y_pred_sklearn, w_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4ae86f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
